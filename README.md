# Methuselah‑prediction (Enhanced)

This repository extends the original Methuselah‑prediction pipeline for yeast chronological lifespan (CLS) and stress survival prediction.  In addition to the UCI yeast demo, it now supports real biological datasets (e.g., Yeast Aging Database or GEO series such as GSE41619), advanced machine‑learning models, interpretability tooling, reproducible workflow management and community‑oriented documentation.

## What's new

 - **Data integration** – The `prepare_data.py` script recognises multiple data types via the `task.data_type` setting.  For tabular gene‑expression matrices (`.csv`, `.tsv`), it computes gene‑wise z‑scores and can optionally derive pathway enrichment scores using the [g:Profiler API](https://biit.cs.ut.ee/gprofiler/gost).  Network‑centric features (degree centrality) can be extracted from protein–protein interaction networks via STRING and `networkx`.  Support for GEO accessions using `GEOparse` is included; when provided with a GEO accession it will attempt to download and parse the series matrix file.

   The pipeline now also accepts *replicative* and *chronological* lifespan datasets derived from large‑scale yeast deletion screens.  To use the **Replicative Lifespan (RLS)** data, download the supplementary tables S1/S2 from the study *“A comprehensive analysis of replicative lifespan in 4,698 single‑gene deletion mutants uncovers conserved mechanisms of aging”* and place them under `data/raw`.  These tables list all tested strains and the subset of long‑lived deletions【864702090041242†L930-L939】.  Optionally, gene‑expression data from GEO accession GSE37241 can be merged by the user.  For **Chronological Lifespan (CLS)** experiments, download the full dataset (survival profiles and CFU estimates) from the chemogenomics website referenced in the PLoS Genetics paper; all supplementary data are available from this page【410200973186345†L1080-L1083】.  Set `task.data_type: rls` or `cls` in `configs/base.yaml` and specify the column containing the lifespan measurement via `task.target` (e.g., `mean_RLS` or `area_under_curve`).  During processing, the script will read the table, compute z‑scores for numeric features and, if enabled, derive pathway and network features.  This alignment with published geroscience datasets enables predictions such as “how does *SIR2* overexpression affect CLS under rapamycin?” and links the project to the broader ageing literature.

   **Replacing the UCI demo with realistic CLS data** – To address the original mismatch between stated goals and implementation, the UCI Yeast dataset has been deprecated in favour of real chronological lifespan measurements.  The configuration now exposes a `problem_type` field; setting `problem_type: survival` instructs the pipeline to treat the outcome as a time‑to‑event variable rather than a categorical label.  Users can provide a duration column (`task.target_time`) and an event indicator (`task.target_event`) to support censored observations.  Chronological lifespan is quantified by plating serially diluted cultures and counting colony forming units (CFUs) over time; CFU counts peak after 2–3 days and serve as the 100 % survival reference【286526631398948†L262-L274】.  These measurements are converted into survival times and censoring flags during preprocessing.  

   A new **data ingestion script**, `fetch_biodata.py`, automates the retrieval of public GEO datasets relevant to CLS.  For example, the caloric restriction and rapamycin dataset **GSE100166** contains gene‑expression profiles of yeast under nutrient perturbations; the script uses `GEOparse` to download the series matrix, extract numeric expression tables and sample metadata, and save them to `data/raw`.  Researchers can merge these features with survival outcomes from deletion screens to build realistic models of aging under stress.  This automated ingestion ensures alignment between the *in silico* models and the nutrient signalling experiments described in the literature.

   The latest iteration also introduces **multi‑omics integration** and **cross‑species ortholog mapping**.  When `task.data_type` is set to `multiomics`, multiple expression/proteomics/metabolomics matrices can be merged on a shared identifier (e.g., gene or sample ID) before z‑scoring.  This allows joint learning across transcriptomic, proteomic and metabolomic layers, paving the way for systems‑level models of ageing.  Cross‑species mapping leverages g:Profiler’s g:Orth/g:Convert services to translate yeast genes to orthologs in other organisms【151163149604280†L400-L415】; this enables enrichment analysis in the human context and facilitates comparative studies.  These features are configurable via `include_ortholog_mapping` and `ortholog_target_species` in the YAML file.

* **Temporal, single‑cell and generative modelling** – New advanced models have been added under `advanced_models.py`: a *TemporalRNN* (LSTM) for time‑series gene expression or survival trajectories, a *VariationalAutoencoder* (VAE) and *ConditionalGAN* for simulating gene‑expression profiles under novel interventions, and a *MetaLearner* wrapper illustrating how to implement meta‑learning or transfer learning.  These classes provide basic network architectures using PyTorch; training loops should be implemented by the user.  Additional data types `scrna` and `spatial` enable ingestion of single‑cell and spatial transcriptomics matrices via `prepare_data.py`.  Together, these additions lay the groundwork for modelling cell‑to‑cell heterogeneity, capturing dynamic ageing processes and predicting the effects of untested gene or drug perturbations.

- **Advanced models** – The `advanced_models.py` module includes working implementations (when optional dependencies are installed) of survival analysis via Cox proportional hazards (`lifelines`) and accelerated failure time models, graph neural networks (`torch_geometric`) and transformers (`transformers`).  A new balanced random forest model (from `imbalanced‑learn`) is exposed to handle imbalanced class distributions.  Hyperparameter optimisation can be performed with Optuna using the `optuna_search.py` script.  Survival modelling is fully integrated into the training workflow: when `task.problem_type` is set to `survival` the script constructs either a Cox proportional hazards model or a random survival forest, fits it on the duration/event data and computes concordance and integrated Brier scores【974672309982697†L144-L156】.
 - **Methodological sophistication** – The `prepare_data.py` script now supports batch effect correction (ComBat), variance stabilisation, missing data imputation, dimensionality reduction (PCA or variational autoencoders) and class balancing via SMOTE.  A new `train_nested_cv.py` module demonstrates how to run nested cross‑validation with hyperparameter tuning using `joblib` for parallel execution; this provides a template for scaling experiments to larger datasets or multiple cores.  Generative augmentation via variational autoencoders and conditional GANs is available through the models in `advanced_models.py`, and the framework is designed to plug in causal inference (`causal_analysis.py`) and fairness auditing (`fairness_audit.py`) without major changes.

- **Interpretability and biological context** – `interpret_shap.py` supports grouping features into biological modules (e.g., KEGG pathways).  The optional `--group-config` argument accepts a YAML mapping of group names to feature names; mean absolute SHAP values are then aggregated per group and plotted.  An `--interactive` flag produces a Plotly HTML chart for interactive exploration of feature importances.  A new `posthoc_analysis.py` script performs simple PubMed queries against top features to suggest relevant literature or GO/KEGG terms.  A complementary `fairness_audit.py` module leverages `aif360` to compute bias metrics (demographic parity, equal opportunity and average odds differences) across yeast strains or experimental conditions.

- **Reproducibility and scalability** – A `Snakefile` orchestrates the full pipeline from data download through training and evaluation, enabling parallel execution and clear dependency management.  All random processes accept a configurable `seed`.  A `dvc.yaml` (not included here) can be added to version control large datasets.  Example Singularity and Docker container recipes are provided in the `containers/` directory for deployment on HPC systems.  The configuration file `configs/base.yaml` exposes additional knobs (e.g., toggling enrichment or network features) and can be extended for new experiments.

- **Documentation, testing and roadmap** – The repository has been reorganised as a Python package (`src/` is importable), enabling use with Jupyter Book.  A growing set of notebooks and tutorials live under `docs/`.  New tests in `tests/` ensure robustness to high‑dimensional data and multicollinearity.  A `ROADMAP.md` captures planned milestones (v1.0: CLS integration, v1.1: survival analysis, etc.).  Ethical considerations around translation of yeast findings to mammals are discussed in the README and notebooks.

See `ROADMAP.md` for a chronological list of future improvements and open issues.